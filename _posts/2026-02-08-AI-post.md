---
layout: post
title: I Treated AI Like It Could Think
date: 2026-02-08
description: Like many developers in my era, I started coding with AI believing I could build almost anything with it. You might think, “Well, I actually can,” but I challenge you to build something you don’t understand.
tags: [technology]
---

In late 2022, OpenAI launched ChatGPT, and it became one of the most talked-about technologies at that time. You could ask it almost anything, and it would respond with something structured, informed, and confident. It felt reasonable to assume that if it could explain things so well, it could also think through tasks for you. And to some extent, it actually can.

I remember using it for the first time to debug some Python code. I was blown away because not only did it give me answers, but it also suggested refinements I could try, and they surprisingly worked. My usual process would have been documentation, <a href="https://stackoverflow.com/" target="_blank">Stack Overflow</a> threads, or piecing together answers from random blog posts. But everything was delivered to me on a plate after a single prompt, so I started relying on it to write small blocks of code, then full functions, and eventually entire files.

I then began thinking I could use it to learn mobile development and build more apps, skipping all the tutorials and documentation. Instead of reading and understanding the fundamentals, I would just describe what I wanted and start building from there. I assumed that because I had a strong background in web development, it wouldn’t be that difficult to transition. I thought I could simply ask the model to translate my web development concepts into iOS terms.

Although I followed the instructions carefully, it didn’t work. At first, I thought: “Yeah, it does that sometimes. I’ll just paste the error message and ask what’s wrong. That had worked before.” A few hours passed, and I was still stuck. I was frustrated because, unlike Python or JavaScript, I didn’t understand anything about what was happening. I realized I would have to learn properly how iOS development actually works before coming back to AI for debugging.

I came across the same issue not long ago. When I first discovered <a href="https://jekyllrb.com/" target="_blank">Jekyll</a>, I decided to use it to build my portfolio. It seemed very much like a “work smart, not hard” approach. I found the <a href="https://github.com/alshedivat/al-folio" target="_blank">al-folio</a> template, customized it, and everything looked fine locally. But once it went live, the styling and JavaScript broke.

So I went to AI to debug it. I spent about five hours trying three different models. Each suggestion led to more changes. Each change led to more issues. Eventually, I had to revert everything and told myself, “Maybe I just have to stop being lazy and go through the documentation, learn how Jekyll actually works at this point.”

Here’s what those experiences taught me: at the end of the day, the current AI systems are just large language models. They aren’t meant to think; they generate. You still need enough understanding to prompt properly, judge results, and know what to change. What it does is amplify what you already understand. It can reduce the workload, but it doesn’t remove the need to master the task you’re asking it to help with. You can fake it at a surface level, but as soon as things get complex, your foundation (or lack of it) becomes obvious.

I’m still learning how to use it better. Maybe in the near future it will evolve far beyond what it is today. But for now, this is where it stands for me.
